# Import necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchtext.datasets import WikiText2
from torchtext.data.utils import get_tokenizer
from collections import Counter
import math
import time
from typing import Dict, List, Tuple
import numpy as np
from tqdm import tqdm

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def split_heads(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.size(0)
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, 
                mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)
        
        # Linear projections and split heads
        Q = self.split_heads(self.W_q(query))
        K = self.split_heads(self.W_k(key))
        V = self.split_heads(self.W_v(value))
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        context = torch.matmul(attention_weights, V)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        output = self.W_o(context)
        return output, attention_weights

class FeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.dropout(F.relu(self.linear1(x)))
        x = self.linear2(x)
        return x

class EncoderLayer(nn.Module):
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:
        # Self attention
        attn_output, attention_weights = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed forward
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x, attention_weights

class DocumentRetriever:
    def __init__(self, documents: List[str]):
        self.documents = documents
        self.doc_embeddings = None
        
    def compute_embeddings(self, model: nn.Module):
        # Simple TF-IDF like embedding for demonstration
        # In practice, you'd use better embedding methods
        vocab = Counter()
        for doc in self.documents:
            vocab.update(doc.split())
            
        self.vocab = {word: idx for idx, (word, _) in enumerate(vocab.most_common())}
        self.doc_embeddings = torch.zeros(len(self.documents), len(self.vocab))
        
        for i, doc in enumerate(self.documents):
            for word in doc.split():
                if word in self.vocab:
                    self.doc_embeddings[i, self.vocab[word]] += 1
                    
        # Normalize embeddings
        self.doc_embeddings = F.normalize(self.doc_embeddings, p=2, dim=1)
        
    def retrieve(self, query: str, k: int = 5) -> List[str]:
        # Create query embedding
        query_embedding = torch.zeros(len(self.vocab))
        for word in query.split():
            if word in self.vocab:
                query_embedding[self.vocab[word]] += 1
                
        query_embedding = F.normalize(query_embedding.unsqueeze(0), p=2, dim=1)
        
        # Compute similarities
        similarities = torch.mm(query_embedding, self.doc_embeddings.t())
        _, indices = similarities[0].topk(k)
        
        return [self.documents[idx] for idx in indices]

class SLM(nn.Module):
    def __init__(self, vocab_size: int, d_model: int, num_heads: int, num_layers: int, 
                 d_ff: int, max_seq_length: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        
        # Token embedding
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, dropout, max_seq_length)
        
        # Encoder layers
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # Output layer
        self.output_layer = nn.Linear(d_model, vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        # Token embedding and positional encoding
        x = self.token_embedding(x) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)
        
        attention_weights = []
        
        # Encoder layers
        for encoder_layer in self.encoder_layers:
            x, attn_weights = encoder_layer(x, mask)
            attention_weights.append(attn_weights)
        
        # Output layer
        output = self.output_layer(x)
        
        return output, attention_weights

class WikiTextDataset(Dataset):
    def __init__(self, data_iter, vocab, seq_length):
        self.data = []
        self.seq_length = seq_length
        self.vocab = vocab
        
        text = []
        for line in data_iter:
            if line.strip():
                text.extend(line.strip().split())
        
        # Convert text to indices
        self.data = [self.vocab[token] if token in self.vocab else self.vocab['<unk>'] 
                    for token in text]
        
    def __len__(self):
        return len(self.data) - self.seq_length
        
    def __getitem__(self, idx):
        return (torch.tensor(self.data[idx:idx+self.seq_length]),
                torch.tensor(self.data[idx+1:idx+self.seq_length+1]))

def train_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, 
                optimizer: torch.optim.Optimizer, device: torch.device) -> float:
    model.train()
    total_loss = 0
    
    for batch_idx, (src, tgt) in enumerate(tqdm(dataloader)):
        src, tgt = src.to(device), tgt.to(device)
        
        optimizer.zero_grad()
        
        # Create mask for padding
        src_mask = (src != 0).unsqueeze(-2)
        
        output, _ = model(src, src_mask)
        loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        
        total_loss += loss.item()
        
    return total_loss / len(dataloader)

def evaluate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, 
            device: torch.device) -> float:
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for src, tgt in dataloader:
            src, tgt = src.to(device), tgt.to(device)
            src_mask = (src != 0).unsqueeze(-2)
            
            output, _ = model(src, src_mask)
            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))
            
            total_loss += loss.item()
            
    return total_loss / len(dataloader)

def main():
    # Hyperparameters
    BATCH_SIZE = 32
    SEQ_LENGTH = 35
    D_MODEL = 256
    NUM_HEADS = 8
    NUM_LAYERS = 4
    D_FF = 1024
    DROPOUT = 0.1
    NUM_EPOCHS = 10
    LEARNING_RATE = 0.0001
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load and preprocess data
    tokenizer = get_tokenizer('basic_english')
    train_iter = WikiText2(split='train')
    val_iter = WikiText2(split='valid')
    test_iter = WikiText2(split='test')
    
    # Build vocabulary
    counter = Counter()
    for line in train_iter:
        counter.update(tokenizer(line))
        
    vocab = {'<unk>': 0, '<pad>': 1}
    vocab.update({word: idx + 2 for idx, (word, count) in 
                 enumerate(counter.most_common()) if count >= 5})
    
    # Create datasets
    train_dataset = WikiTextDataset(WikiText2(split='train'), vocab, SEQ_LENGTH)
    val_dataset = WikiTextDataset(WikiText2(split='valid'), vocab, SEQ_LENGTH)
    test_dataset = WikiTextDataset(WikiText2(split='test'), vocab, SEQ_LENGTH)
    
    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
    
    # Initialize model
    model = SLM(
        vocab_size=len(vocab),
        d_model=D_MODEL,
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS,
        d_ff=D_FF,
        max_seq_length=SEQ_LENGTH,
        dropout=DROPOUT
    ).to(device)
    
    # Initialize retriever for RAG
    documents = []
    for line in WikiText2(split='train'):
        if line.strip():
            documents.append(line.strip())
    retriever = DocumentRetriever(documents)
    retriever.compute_embeddings(model)
    
    # Training setup
    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)
    
    # Training loop
    best_val_loss = float('inf')
    
    for epoch in range(NUM_EPOCHS):
        start_time = time.time()
        
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss = evaluate(model, val_loader, criterion, device)
        
        # Learning rate scheduling
        scheduler.step(val_loss)
        
        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pt')
        
        elapsed = time.time() - start_time
        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | '
              f'Time: {elapsed:.1f}s')
        
        # Example of using RAG
        query = "artificial intelligence"
        retrieved_docs = retriever.retrieve(query)
        print(f"\nRetrieved documents for query '{query}':")
        for i, doc in enumerate(retrieved_docs, 1):
            print(f"{i}. {doc[:100]}...")
    
    # Final evaluation
    model.load_state_dict(torch.load('best_model.pt'))
    test_loss = evaluate(model, test_loader, criterion, device)
    print(f'\nTest Loss: {test_loss:.3f}')

if __name__ == '__main__':
    main()
